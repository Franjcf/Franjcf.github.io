---
layout: default
modal-id: 6
date: 2014-07-15
img: gradient_decent.png
alt: image-alt
project-date: April 2014
client: Start Bootstrap
category: Web Development
description: 
Summary: In this mini-project I code gradient decesnt from scratch to solve linear regression and ridge regularization problems. These particular problems were chosen becauese their analytical solutions are well-known. Furthermore I investigate how the gradient step size affects the rate of convergence of the underlying optimization problem. I then procced by calculating the largest and smallest eigenvalues of the second derivative of objective function in order to set optimal step size and to find the lower bound the rate of convergence. Finally I investigate how the regularization term "lambda" affects said convergence rate.

Skills: Vector Calculus, Linear Algebra, Optimization, Gradient Decent, Python, Ridge and Linear Regression

Link to Notebook: https://github.com/Franjcf/Data-Science-Projects/blob/main/gradient_decent_optimization_and_implementation/unconstrained_optimizationwith_gradient_decent.ipynb
---
